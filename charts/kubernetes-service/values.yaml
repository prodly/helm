# Default values for your service.
#
# It is expected to provide specific values for this file either by:
# 1. [RECOMMENDED] Override particular files in Helm chart using Skaffold profiles.
# 2. Copying the file by itself into service repository and modify its content.
#
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

# This is the version number of the application being deployed. This version number should be
# incremented each time you make changes to the application. Versions are not expected to
# follow Semantic Versioning. They should reflect the version the application is using.
# It is recommended to use it with quotes.
appVersion: "1.0.0"

# This section describes Docker image properties for your service
image:
  repository: "YOUR_SERVICE_DOCKER_IMAGE_TAG"
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "latest"

# THis section describes properties of your service itself
service:
  type: NodePort
  port: "YOUR_SERVICE_PORT"
  path: "YOUR_SERVICE_CONTEXT_PATH"
  customHealthCheckPath: ""
  sg_role: ""

sqsd:
  enabled: false
  repository: "YOUR_SERVICE_DOCKER_IMAGE_TAG"
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "latest"
  queueName: "WORKER_QUEUE_NAME"
  queueRegion: "WORKER_QUEUE_REGION"
  maxMessagesPerRequest: 10
  runDaemonized: true
  sleepTime: 10
  waitTime: 10
  # workerHttpHost is set with protocol (http:// | https://)
  workerHttpHost: "WORKER_HTTP_HOSTNAME"
  # Worker app endpoint
  workerHttpPath: "/"
  workerContentType: "application/json"
  serviceAccount: "SQSD_SERVICE_ACCOUNT"

# Permissions section
serviceAccount: ""

ingress:
  enabled: true
  className: "nginx"
  annotations: {}
  hosts:
    - host: "YOUR_SERVICE_EXTERNAL_HOST_NAME_FROM_ROUTE_53"
      paths:
        - path: "/YOUR_SERVICE_CONTEXT_PATH"
          pathType: Prefix

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 1
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Defines variety of rollout strategies Kubernetes can offer alongside with ArgoCD
rollout:

  # All existing pods will be killed before new ones are created
  recreate:
    enabled: false

  # Gradually replace pods with newer versions to ensure that there are enough new pods to maintain the availability threshold
  rollingUpdate:
    enabled: false
    # Defines maximum number of pods that can be unavailable during update process
    #maxUnavailable: 25%
    # Defines maximum number of pods that can be created over thr desired number of pods
    #maxSurge: 25%

  # Release a new version to a small percentage of the production traffic
  canary:
    enabled: false
    # Defines maximum number of pods that can be unavailable during update process (default - 25%)
    #maxUnavailable: 25%
    # Defines maximum number of pods that can be created over thr desired number of pods (default - 25%)
    #maxSurge: 25%
    # List of Canary steps
    steps:
      - setWeight: 33
      - pause:
          duration: 30s
      - setWeight: 66
      - pause:
          duration: 30s

  # Recreate new complete environment before switching traffic
  blueGreen:
    enabled: false
    # autoPromotionEnabled disables automated promotion of the new stack by pausing the rollout
    # immediately before the promotion. If omitted, the default behavior is to promote the new
    # stack as soon as the ReplicaSet are completely ready/available.
    # Rollouts can be resumed using: `kubectl argo rollouts promote ROLLOUT`
    autoPromotionEnabled: false
    # Make the rollout automatically promote deployment once new set of service is healthy after N seconds
    #autoPromotionSeconds: 600
    # Delays scale down the old ReplicaSet after the active Service is switched to the new ReplicaSet (default - 30)
    #scaleDownDelaySeconds: 1800
    # Limits the number of old active ReplicaSets to keep scaled up while they wait for the scaleDownDelay to pass after being removed from the active service
    #scaleDownDelayRevisionLimit: 15
    # Number of replicas that the new version of an application should run (default - 100% of active replicas)
    #previewReplicaCount: 1
    # Configures Analysis before it switches traffic to the new version.
    enablePrePromotionAnalysis: true
    enablePostPromotionAnalysis: true

  # Sets properties for progressive delivery analysis during rollout
  analysis:
    # Do not start analysis until this amount of time after the analysis run starts
    #initialDelay: 1m
    # Interval between analysis queries
    #interval: 1m
    # Amount of queries to perform during analysis
    #count: 5
    # Percentage of successful requests to consider analysis as successful (default - 0.95)
    successCondition: 0.95
    # Amount of failures that are considered sufficient to mark analysis as failed (default - 3)
    failureLimit: 1
    # Delay starting analysis run until certain weight step (default - 1)
    startingStep: 1

  # Extra AnalysisTemplate objects
  extraAnalysisTemplates: {}

# https://kubernetes.io/docs/tasks/run-application/configure-pdb/
#
# minAvailable - When you specify an integer, it represents a number of Pods.
# For instance, if you set minAvailable to 10, then 10 Pods must always be available,
# even during a disruption.
#
# maxUnavailable When you specify a percentage by setting the value to a string representation
# of a percentage (eg. "50%"), it represents a percentage of total Pods. For instance, if you set
# maxUnavailable to "50%", then only 50% of the Pods can be unavailable during a disruption.
disruptionBudget:
  enabled: false
  spec:
    maxUnavailable: 49%

# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
# the affinity allowing you to constrain which nodes your Pod can be scheduled on based
# on node labels. There are two types of node affinity:
#   requiredDuringSchedulingIgnoredDuringExecution: The scheduler can't schedule the Pod unless the rule is met.
#   This functions like nodeSelector, but with a more expressive syntax.
#
#   preferredDuringSchedulingIgnoredDuringExecution: The scheduler tries to find a node that meets
#   the rule. If a matching node is not available, the scheduler still schedules the Pod.
#
# You can specify a weight between 1 and 100 for each instance of the
# preferredDuringSchedulingIgnoredDuringExecution affinity type. When the scheduler finds nodes
# that meet all the other scheduling requirements of the Pod, the scheduler iterates through every
# preferred rule that the node satisfies and adds the value of the weight for that expression to a sum.

#affinity:
#  podAntiAffinity:
#    preferredDuringSchedulingIgnoredDuringExecution:
#      - weight: 100
#        podAffinityTerm:
#          labelSelector:
#            matchExpressions:
#              - key: "app.kubernetes.io/name"
#                operator: In
#                values:
#                  - "kubernetes-service"
#          topologyKey: "kubernetes.io/hostname"

#nodeSelector:
#  name: "backend"